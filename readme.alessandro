Let me explain it in a top-down manner:
In principle everything is done by a bash script called "robot.sh"
_________________________________________________________________

yourfilenames=`ls ./benchmark_alessandro/*.spn`
for eachfile in $yourfilenames
do
   filename="${eachfile%.*}"
   echo $eachfile
   python binarizer.py --spn_file $filename.spn
   python spn2milp.py --spn_file $filename.spn2l --order 'DENIS'
   python milp2map.py --lp_file $filename.lp --map_file $filename.map --spn_file $filename.spn2l --output_file $filename.result
done

_________________________________________________________________

The file browses in a folder ('benchmark_alessandro') for the .spn files
These .spn files are exactly the ones in the Heitor's 'learned-spn' repository
What the robot is doing is:
1) binarize the network, this creates a file with two files with extensions .spn2 and .spn2l (not the best solution I know, but be patient with me :))
2) read the .spn2l file and create a (generic, i.e., not specific for the query/evidence task) .lp file
3) read the .lp file together with a .map file which contains info about query and evidence in a single file in the same way Heitor is doing in learned-spn (note only the first two lines are used)

The output with information about the bounds on the solution are in the .result file
__________________________________________________________________

A few remarks:

- I already executed the "binarization" part for all the networks and the "spn2milp" part for most of the files (running the big ones right now).
- You can find in "benchmark_alessandro" all the .spn2l files and most of the .lp files
- So for your simulation you just need to run the robot with the first two python lines commented
- Before doing that check that, for each network, you have there: (1) the .spn file as in your repo; (2) the .spn2l generated by my code; (3) the .lp file generated by my code; (4) the .map file as in your repo.
- In line 32 of spn2milp.py the code need an overestimate for the number of nodes, I use 60'000 (default) for the small ones but you should change it to 600'000 for large ones
- I moved my python files to the main folder and the data in the 'benchmark_alessandro' subfolder (I know it is a bit messy, but was faster this way)

As soon as other .lp files will be ready, I'll let you know
You please let me know when your .map files are definitive and if the bounds are copied in the paper (I can run the same queries with milp to check the values for debug)


